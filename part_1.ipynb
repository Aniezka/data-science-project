{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "\n",
    "This part contains code for scraping Wikipedia biographies of physicists and philosophers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://en.wikipedia.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_philo_list_links():\n",
    "    \"\"\"Returns a list of links which are list of philosophers by period.\"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/Category:Lists_of_philosophers_by_period\"\n",
    "\n",
    "    # Get base page\n",
    "    page = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Get all links in the ul\n",
    "    philo_pages_link = soup.select(\".mw-category-group ul li a\")\n",
    "\n",
    "    # Extract the urls\n",
    "    philo_pages_url = [BASE_URL + link.get('href') for link in philo_pages_link]\n",
    "\n",
    "    return philo_pages_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_philo_links(list_urls):\n",
    "    \"\"\"Return a list of links which are about philosophers.\"\"\"\n",
    "    urls = []\n",
    "    for url in list_urls:\n",
    "        # Get list page\n",
    "        page = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Get all links in the ul\n",
    "        # Exclude a tags with class new to avoid red links\n",
    "        # Exclude last ul which is see also\n",
    "        pages_link = soup.select(\".mw-parser-output > ul:not(:last-of-type) > li > a:not(.new)\")\n",
    "\n",
    "        # Extract the urls\n",
    "        pages_url = [BASE_URL + link.get('href') for link in pages_link]\n",
    "        urls.extend(pages_url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_physi_list_links():\n",
    "    \"\"\"Returns a list of links which are list of physicists by century.\"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/Category:Physicists_by_century\"\n",
    "\n",
    "    # Get base page\n",
    "    page = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Get first level (category) links\n",
    "    pages_link = soup.select(\".mw-category-group:last-of-type > ul > li > .CategoryTreeSection > .CategoryTreeItem > a\")\n",
    "    pages_url = [BASE_URL + link.get('href') for link in pages_link]\n",
    "\n",
    "    # Get second level (individual) links\n",
    "    physi_list_urls = []\n",
    "    for page_url in pages_url:\n",
    "        page = requests.get(url=page_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        pages_link = soup.select(\".mw-category-group > ul > li > .CategoryTreeSection > .CategoryTreeItem > a\")\n",
    "        physi_list_urls.extend([BASE_URL + link.get('href') for link in pages_link])\n",
    "\n",
    "    return physi_list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_physi_links(list_urls):\n",
    "    \"\"\"Return a list of links which are about physicists.\"\"\"\n",
    "    urls = []\n",
    "    for url in list_urls:\n",
    "        # Get list page\n",
    "        page = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Get all links in the ul\n",
    "        # Exclude a tags with class new to avoid red links\n",
    "        pages_link = soup.select(\".mw-category > .mw-category-group > ul > li > a:not(.new)\")\n",
    "\n",
    "        # Extract the urls\n",
    "        pages_url = [BASE_URL + link.get('href') for link in pages_link]\n",
    "        urls.extend(pages_url)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_title_content(url):\n",
    "    \"\"\"Return the title and content of a wikipedia page.\"\"\"\n",
    "    page = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Get the title of the page\n",
    "    title = soup.select(\"#firstHeading\")[0].text\n",
    "\n",
    "    # Get the content of the page\n",
    "    content = soup.select(\"#mw-content-text > div.mw-parser-output > p\")\n",
    "\n",
    "    # Extract the text\n",
    "    text = [p.text for p in content]\n",
    "\n",
    "    # join with space, remove \\n and \\xa0, remove [digits], remove {style settings}\n",
    "    text = \" \".join(text).replace(\"\\n\", \"\").replace(\"\\xa0\", \" \").strip()\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\{.*\\}\", \"\", text)\n",
    "\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt(title, category, text):\n",
    "    \"\"\"Write the text to a txt file.\"\"\"\n",
    "    # Format the title\n",
    "    title = title.title().replace(\" \", \"\")\n",
    "    title = re.sub(r\"\\(.*\\)\", \"\", title)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(f\"./{category}\", exist_ok=True)\n",
    "\n",
    "    with open(f\"./{category}/{title}_{category}.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(list_urls, category):\n",
    "    \"\"\"\n",
    "    Write a txt file for each url.\n",
    "    Return a set of texts.\n",
    "    \"\"\"\n",
    "    texts = set()\n",
    "    for url in list_urls:\n",
    "        # Get title and page content\n",
    "        title, text = get_page_title_content(url)\n",
    "\n",
    "        # Add to the set\n",
    "        texts.add(text)\n",
    "\n",
    "        # Write a txt file\n",
    "        write_txt(title, category, text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping\n",
    "philo_list_urls = get_philo_list_links()\n",
    "philo_urls = get_philo_links(philo_list_urls)\n",
    "\n",
    "physi_list_urls = get_physi_list_links()\n",
    "physi_urls = get_physi_links(physi_list_urls)\n",
    "\n",
    "philo_texts = get_texts(philo_urls, \"Philosopher\")\n",
    "physi_texts = get_texts(physi_urls, \"Physicist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with text-category pairs\n",
    "data = [(text, \"Philosopher\") for text in philo_texts] + \\\n",
    "       [(text, \"Physicist\") for text in physi_texts]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"category\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
